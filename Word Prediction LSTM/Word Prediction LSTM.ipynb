{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a 2 layer LSTM on a PTB dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning schedule\n",
    "- 1.42 initial learning rate\n",
    "- Divided by 1.4 every epoch\n",
    "- Additional division by 1.4 on epochs 4,5,8,11,15\n",
    "\n",
    "#### Hyperparameters\n",
    "- Initial batch size 20\n",
    "- Hidden layer size 560"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import time\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device= torch.device(\"cuda\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([46479, 20])\n",
      "torch.Size([4121, 20])\n"
     ]
    }
   ],
   "source": [
    "train_data  =  torch.load('../data/ptb/train_data.pt')\n",
    "test_data   =  torch.load('../data/ptb/test_data.pt')\n",
    "\n",
    "print(  train_data.size()  )\n",
    "print(  test_data.size()   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 20\n",
    "\n",
    "vocab_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class three_layer_recurrent_net(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer1 = nn.Embedding( vocab_size  , hidden_size  )\n",
    "        self.layer2 = nn.LSTM(      hidden_size , hidden_size, num_layers=2, bidirectional=True  )\n",
    "        self.layer3 = nn.Linear(    hidden_size*2 , vocab_size   )\n",
    "\n",
    "        \n",
    "    def forward(self, word_seq ):\n",
    "        \n",
    "        g_seq                     =   self.layer1( word_seq )  \n",
    "        h_seq , (h_fin,c_fin)     =   self.layer2( g_seq )\n",
    "        \n",
    "        h_direc_1  = h_fin[2,:,:]\n",
    "        h_direc_2  = h_fin[3,:,:]\n",
    "        h_direc_12 = torch.cat( (h_direc_1, h_direc_2)  , dim=1) \n",
    "        \n",
    "        score_seq                 =   self.layer3( h_seq )\n",
    "        \n",
    "        return score_seq,  (h_fin,c_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "three_layer_recurrent_net(\n",
      "  (layer1): Embedding(10000, 560)\n",
      "  (layer2): LSTM(560, 560, num_layers=2, bidirectional=True)\n",
      "  (layer3): Linear(in_features=1120, out_features=10000, bias=True)\n",
      ")\n",
      "There are 29371920 (29.37 million) parameters in this neural network\n"
     ]
    }
   ],
   "source": [
    "hidden_size= 560\n",
    "\n",
    "net = three_layer_recurrent_net( hidden_size )\n",
    "\n",
    "print(net)\n",
    "\n",
    "utils.display_num_param(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0608, -0.0990,  0.0603,  ...,  0.0214, -0.0137,  0.0947],\n",
       "        [-0.0317,  0.0665,  0.0983,  ...,  0.0804,  0.0502,  0.0894],\n",
       "        [-0.0230,  0.0614, -0.0191,  ...,  0.0858, -0.0075, -0.0081],\n",
       "        ...,\n",
       "        [-0.0021, -0.0790,  0.0028,  ...,  0.0892,  0.0578, -0.0505],\n",
       "        [-0.0770,  0.0289,  0.0382,  ...,  0.0613, -0.0875,  0.0640],\n",
       "        [ 0.0192,  0.0602, -0.0744,  ...,  0.0487, -0.0022,  0.0673]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.layer1.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "net.layer3.weight.data.uniform_(-0.1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "my_lr = 1.42\n",
    "\n",
    "seq_length = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_on_test_set():\n",
    "\n",
    "    running_loss=0\n",
    "    num_batches=0    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "       \n",
    "        h = torch.zeros(1, bs, hidden_size)\n",
    "\n",
    "        h=h.to(device)\n",
    "\n",
    "\n",
    "        for count in range( 0 , 4120-seq_length ,  seq_length) :\n",
    "\n",
    "            minibatch_data =  test_data[ count   : count+seq_length   ]\n",
    "            minibatch_label = test_data[ count+1 : count+seq_length+1 ]\n",
    "\n",
    "            minibatch_data=minibatch_data.to(device)\n",
    "            minibatch_label=minibatch_label.to(device)\n",
    "\n",
    "            scores, h  = net( minibatch_data)\n",
    "            \n",
    "            minibatch_label =   minibatch_label.view(  bs*seq_length ) \n",
    "            scores          =            scores.view(  bs*seq_length , vocab_size)\n",
    "\n",
    "            loss = criterion(  scores ,  minibatch_label )    \n",
    "\n",
    "            running_loss += loss.item()\n",
    "            num_batches += 1        \n",
    "    \n",
    "    total_loss = running_loss/num_batches \n",
    "    print('test: exp(loss) = ', math.exp(total_loss)  )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch= 0 \t time= 26.305980443954468 \t lr= 1.42 \t exp(loss)= 16.727946023250478\n",
      "test: exp(loss) =  3.7215441238313307\n",
      "\n",
      "epoch= 1 \t time= 53.18167996406555 \t lr= 1.42 \t exp(loss)= 2.5461094258427304\n",
      "test: exp(loss) =  2.0729269419200125\n",
      "\n",
      "epoch= 2 \t time= 80.21796131134033 \t lr= 1.42 \t exp(loss)= 1.7046225477939347\n",
      "test: exp(loss) =  1.6875763200311193\n",
      "\n",
      "epoch= 3 \t time= 107.12412643432617 \t lr= 1.42 \t exp(loss)= 1.4449222336263272\n",
      "test: exp(loss) =  1.5516426766441127\n",
      "\n",
      "epoch= 4 \t time= 134.14594197273254 \t lr= 0.7244897959183674 \t exp(loss)= 1.2896318065707295\n",
      "test: exp(loss) =  1.4684482268082386\n",
      "\n",
      "epoch= 5 \t time= 161.09005284309387 \t lr= 0.3696376509787589 \t exp(loss)= 1.222270736930623\n",
      "test: exp(loss) =  1.454693853759221\n",
      "\n",
      "epoch= 6 \t time= 188.0818543434143 \t lr= 0.26402689355625636 \t exp(loss)= 1.1880529560529531\n",
      "test: exp(loss) =  1.4568458502268484\n",
      "\n",
      "epoch= 7 \t time= 214.9742579460144 \t lr= 0.18859063825446884 \t exp(loss)= 1.164550359211816\n",
      "test: exp(loss) =  1.4602903752917837\n",
      "\n",
      "epoch= 8 \t time= 241.88487911224365 \t lr= 0.09621971339513719 \t exp(loss)= 1.1486010026588875\n",
      "test: exp(loss) =  1.4589829039293616\n",
      "\n",
      "epoch= 9 \t time= 268.8137035369873 \t lr= 0.06872836671081228 \t exp(loss)= 1.1392503191357934\n",
      "test: exp(loss) =  1.4594926028507638\n",
      "\n",
      "epoch= 10 \t time= 295.6803984642029 \t lr= 0.049091690507723065 \t exp(loss)= 1.1328104972650002\n",
      "test: exp(loss) =  1.460153952587591\n",
      "\n",
      "epoch= 11 \t time= 322.71230959892273 \t lr= 0.025046780871287283 \t exp(loss)= 1.1292972052771786\n",
      "test: exp(loss) =  1.4593727597428872\n",
      "\n",
      "epoch= 12 \t time= 349.6552188396454 \t lr= 0.017890557765205203 \t exp(loss)= 1.1269947061401322\n",
      "test: exp(loss) =  1.4591788939503065\n",
      "\n",
      "epoch= 13 \t time= 376.6361424922943 \t lr= 0.012778969832289431 \t exp(loss)= 1.1257379111067267\n",
      "test: exp(loss) =  1.4589309454955937\n",
      "\n",
      "epoch= 14 \t time= 403.51329469680786 \t lr= 0.009127835594492451 \t exp(loss)= 1.1251433495178442\n",
      "test: exp(loss) =  1.4586296278383946\n",
      "\n",
      "epoch= 15 \t time= 430.42563581466675 \t lr= 0.004657058976781863 \t exp(loss)= 1.1258928187155681\n",
      "test: exp(loss) =  1.4584533368897266\n",
      "\n",
      "epoch= 16 \t time= 457.37750244140625 \t lr= 0.003326470697701331 \t exp(loss)= 1.126746949226393\n",
      "test: exp(loss) =  1.4585348162421634\n",
      "\n",
      "epoch= 17 \t time= 484.2422065734863 \t lr= 0.0023760504983580937 \t exp(loss)= 1.1279243539666126\n",
      "test: exp(loss) =  1.458744931032727\n",
      "\n",
      "epoch= 18 \t time= 511.262122631073 \t lr= 0.0016971789273986385 \t exp(loss)= 1.1293975866321326\n",
      "test: exp(loss) =  1.4587159773268175\n",
      "\n",
      "epoch= 19 \t time= 538.2149176597595 \t lr= 0.001212270662427599 \t exp(loss)= 1.1311294092170163\n",
      "test: exp(loss) =  1.4586043349829945\n",
      "\n",
      "epoch= 20 \t time= 565.2087607383728 \t lr= 0.0008659076160197136 \t exp(loss)= 1.1327644384107605\n",
      "test: exp(loss) =  1.4584379046708453\n",
      "\n",
      "epoch= 21 \t time= 592.0955767631531 \t lr= 0.0006185054400140811 \t exp(loss)= 1.13414792122541\n",
      "test: exp(loss) =  1.4580573655084705\n",
      "\n",
      "epoch= 22 \t time= 619.0018975734711 \t lr= 0.000441789600010058 \t exp(loss)= 1.1352873387249938\n",
      "test: exp(loss) =  1.457659676447892\n",
      "\n",
      "epoch= 23 \t time= 645.9514272212982 \t lr= 0.0003155640000071843 \t exp(loss)= 1.1360790458077383\n",
      "test: exp(loss) =  1.4573252445715912\n",
      "\n",
      "epoch= 24 \t time= 672.8976550102234 \t lr= 0.0002254028571479888 \t exp(loss)= 1.136456881126719\n",
      "test: exp(loss) =  1.4570515785676394\n",
      "\n",
      "epoch= 25 \t time= 699.8514914512634 \t lr= 0.00016100204081999201 \t exp(loss)= 1.1365639392824711\n",
      "test: exp(loss) =  1.4568478488047942\n",
      "\n",
      "epoch= 26 \t time= 726.793771982193 \t lr= 0.00011500145772856574 \t exp(loss)= 1.1365625233856036\n",
      "test: exp(loss) =  1.45670475170847\n",
      "\n",
      "epoch= 27 \t time= 753.7114751338959 \t lr= 8.214389837754696e-05 \t exp(loss)= 1.136529911166674\n",
      "test: exp(loss) =  1.4566061188099277\n",
      "\n",
      "epoch= 28 \t time= 780.6449770927429 \t lr= 5.867421312681926e-05 \t exp(loss)= 1.1364939932605522\n",
      "test: exp(loss) =  1.4565382879637927\n",
      "\n",
      "epoch= 29 \t time= 807.5346558094025 \t lr= 4.1910152233442334e-05 \t exp(loss)= 1.1364631160022478\n",
      "test: exp(loss) =  1.456491416623968\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "\n",
    "for epoch in range(30):\n",
    "    \n",
    "    # keep the learning rate to 1 during the first 4 epochs, then divide by 1.1 at every epoch\n",
    "    if epoch == 4 or epoch == 5 or epoch == 8 or epoch == 11 or epoch == 15:\n",
    "        my_lr = my_lr / 1.4\n",
    "        \n",
    "    if epoch >= 4:\n",
    "        my_lr = my_lr / 1.4\n",
    "        \n",
    "    \n",
    "    # create a new optimizer and give the current learning rate.   \n",
    "    optimizer=torch.optim.SGD( net.parameters() , lr=my_lr )\n",
    "        \n",
    "    # set the running quantities to zero at the beginning of the epoch\n",
    "    running_loss=0\n",
    "    num_batches=0    \n",
    "    \n",
    "    for count in range( 0 , 46478-seq_length ,  seq_length):\n",
    "             \n",
    "        # Set the gradients to zeros\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # create a minibatch\n",
    "        minibatch_data =  train_data[ count   : count+seq_length   ]\n",
    "        minibatch_label = train_data[ count+1 : count+seq_length+1 ]        \n",
    "        \n",
    "        # send them to the gpu\n",
    "        minibatch_data=minibatch_data.to(device)\n",
    "        minibatch_label=minibatch_label.to(device)\n",
    "                       \n",
    "        # forward the minibatch through the net        \n",
    "        scores, h  = net( minibatch_data )\n",
    "        \n",
    "        # reshape the scores and labels to huge batch of size bs*seq_length\n",
    "        scores          =            scores.view(  bs*seq_length , vocab_size)  \n",
    "        minibatch_label =   minibatch_label.view(  bs*seq_length )       \n",
    "        \n",
    "        # Compute the average of the losses of the data points in this huge batch\n",
    "        loss = criterion(  scores ,  minibatch_label )\n",
    "        \n",
    "        # backward pass to compute dL/dR, dL/dV and dL/dW\n",
    "        loss.backward()\n",
    "\n",
    "        # do one step of stochastic gradient descent: R=R-lr(dL/dR), V=V-lr(dL/dV), ...\n",
    "        utils.normalize_gradient(net)\n",
    "        optimizer.step()\n",
    "        \n",
    "            \n",
    "        # update the running loss  \n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "    # compute stats for the full training set\n",
    "    total_loss = running_loss/num_batches\n",
    "    elapsed = time.time()-start\n",
    "    \n",
    "    print('')\n",
    "    print('epoch=',epoch, '\\t time=', elapsed,'\\t lr=', my_lr, '\\t exp(loss)=',  math.exp(total_loss))\n",
    "    eval_on_test_set() \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference\n",
    "* In each cell is a sentence taken from ptb test set. Use utils.text2tensor() to convert this sentence into a LongTensor. \n",
    "* Feed the sentence to the network\n",
    "* The network should compute a probability vector over the full vocabulary of 10,000 words. This vector tells you which words are likely to come next. Display the 30 most likely words according to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prices averaging roughly $ N a barrel higher in the third ... \n",
      "\n",
      "4.9%\t to\n",
      "2.6%\t issue\n",
      "2.5%\t <eos>\n",
      "1.8%\t <unk>\n",
      "1.7%\t new\n",
      "1.5%\t parties\n",
      "1.1%\t it\n",
      "1.0%\t the\n",
      "1.0%\t stock\n",
      "0.9%\t world\n",
      "0.9%\t that\n",
      "0.9%\t assets\n",
      "0.9%\t creditors\n",
      "0.8%\t prices\n",
      "0.8%\t commission\n",
      "0.7%\t big\n",
      "0.7%\t rates\n",
      "0.7%\t issues\n",
      "0.7%\t court\n",
      "0.6%\t center\n",
      "0.6%\t market\n",
      "0.6%\t supply\n",
      "0.5%\t street\n",
      "0.5%\t guard\n",
      "0.5%\t investors\n",
      "0.5%\t fund\n",
      "0.5%\t actions\n",
      "0.5%\t treasury\n",
      "0.5%\t house\n",
      "0.5%\t bonds\n"
     ]
    }
   ],
   "source": [
    "sentence = \"prices averaging roughly $ N a barrel higher in the third\"\n",
    "\n",
    "x = utils.text2tensor(sentence)\n",
    "x = x.unsqueeze(0)\n",
    "\n",
    "scores, (h,c) = net( x.to(device) )\n",
    "\n",
    "print(sentence, '... \\n')\n",
    "\n",
    "p = F.softmax(scores[0][-1],dim=0)\n",
    "\n",
    "utils.show_most_likely_words(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i think my line has been very consistent mrs. hills said at a news ... \n",
      "\n",
      "38.2%\t of\n",
      "19.3%\t <eos>\n",
      "5.6%\t to\n",
      "4.4%\t the\n",
      "2.7%\t and\n",
      "2.4%\t that\n",
      "2.2%\t 's\n",
      "1.4%\t they\n",
      "0.9%\t he\n",
      "0.7%\t with\n",
      "0.7%\t for\n",
      "0.7%\t it\n",
      "0.6%\t from\n",
      "0.6%\t as\n",
      "0.5%\t but\n",
      "0.5%\t a\n",
      "0.5%\t there\n",
      "0.5%\t is\n",
      "0.4%\t in\n",
      "0.4%\t during\n",
      "0.3%\t will\n",
      "0.3%\t on\n",
      "0.3%\t because\n",
      "0.2%\t might\n",
      "0.2%\t expects\n",
      "0.2%\t <unk>\n",
      "0.2%\t since\n",
      "0.2%\t i\n",
      "0.2%\t fell\n",
      "0.2%\t three\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentence = \"i think my line has been very consistent mrs. hills said at a news\"\n",
    "\n",
    "x = utils.text2tensor(sentence)\n",
    "x = x.unsqueeze(0)\n",
    "\n",
    "scores, (h,c) = net( x.to(device) )\n",
    "\n",
    "print(sentence, '... \\n')\n",
    "\n",
    "p = F.softmax(scores[0][-1],dim=0)\n",
    "\n",
    "utils.show_most_likely_words(p)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this appears particularly true at gm which had strong sales in the ... \n",
      "\n",
      "4.3%\t company\n",
      "4.1%\t federal\n",
      "3.9%\t <unk>\n",
      "2.1%\t market\n",
      "1.7%\t board\n",
      "1.4%\t stock\n",
      "1.2%\t new\n",
      "1.2%\t first\n",
      "1.1%\t u.s.\n",
      "1.0%\t same\n",
      "0.9%\t government\n",
      "0.9%\t american\n",
      "0.9%\t most\n",
      "0.9%\t big\n",
      "0.8%\t firm\n",
      "0.7%\t state\n",
      "0.6%\t sale\n",
      "0.6%\t biggest\n",
      "0.6%\t economy\n",
      "0.5%\t issue\n",
      "0.5%\t city\n",
      "0.5%\t house\n",
      "0.5%\t price\n",
      "0.5%\t fed\n",
      "0.5%\t past\n",
      "0.5%\t next\n",
      "0.5%\t gain\n",
      "0.5%\t fact\n",
      "0.5%\t number\n",
      "0.5%\t current\n"
     ]
    }
   ],
   "source": [
    "sentence = \"this appears particularly true at gm which had strong sales in the\"\n",
    "\n",
    "x = utils.text2tensor(sentence)\n",
    "x = x.unsqueeze(0)\n",
    "\n",
    "scores, (h,c) = net( x.to(device) )\n",
    "\n",
    "print(sentence, '... \\n')\n",
    "\n",
    "p = F.softmax(scores[0][-1],dim=0)\n",
    "\n",
    "utils.show_most_likely_words(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some analysts expect oil prices to remain relatively ... \n",
      "\n",
      "5.1%\t high\n",
      "4.0%\t <unk>\n",
      "3.2%\t sales\n",
      "2.0%\t analyst\n",
      "1.4%\t tax\n",
      "0.9%\t manager\n",
      "0.9%\t buying\n",
      "0.8%\t with\n",
      "0.8%\t three\n",
      "0.8%\t at\n",
      "0.8%\t few\n",
      "0.6%\t small\n",
      "0.5%\t civil\n",
      "0.5%\t parts\n",
      "0.5%\t special\n",
      "0.5%\t law\n",
      "0.4%\t widespread\n",
      "0.4%\t trader\n",
      "0.4%\t combined\n",
      "0.4%\t liability\n",
      "0.4%\t months\n",
      "0.4%\t large\n",
      "0.4%\t ahead\n",
      "0.4%\t joint\n",
      "0.4%\t night\n",
      "0.3%\t filed\n",
      "0.3%\t hours\n",
      "0.3%\t friday\n",
      "0.3%\t offering\n",
      "0.3%\t representatives\n"
     ]
    }
   ],
   "source": [
    "sentence = \"some analysts expect oil prices to remain relatively\"\n",
    "\n",
    "x = utils.text2tensor(sentence)\n",
    "x = x.unsqueeze(0)\n",
    "\n",
    "scores, (h,c) = net( x.to(device) )\n",
    "\n",
    "print(sentence, '... \\n')\n",
    "\n",
    "p = F.softmax(scores[0][-1],dim=0)\n",
    "\n",
    "utils.show_most_likely_words(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making 3 original sentences and see what the network predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the trade war is expected to continue to have an effect on the ... \n",
      "\n",
      "4.3%\t company\n",
      "4.1%\t federal\n",
      "3.9%\t <unk>\n",
      "2.1%\t market\n",
      "1.7%\t board\n",
      "1.4%\t stock\n",
      "1.2%\t new\n",
      "1.2%\t first\n",
      "1.1%\t u.s.\n",
      "1.0%\t same\n",
      "0.9%\t government\n",
      "0.9%\t american\n",
      "0.9%\t most\n",
      "0.9%\t big\n",
      "0.8%\t firm\n",
      "0.7%\t state\n",
      "0.6%\t sale\n",
      "0.6%\t biggest\n",
      "0.6%\t economy\n",
      "0.5%\t issue\n",
      "0.5%\t city\n",
      "0.5%\t house\n",
      "0.5%\t price\n",
      "0.5%\t fed\n",
      "0.5%\t past\n",
      "0.5%\t next\n",
      "0.5%\t gain\n",
      "0.5%\t fact\n",
      "0.5%\t number\n",
      "0.5%\t current\n"
     ]
    }
   ],
   "source": [
    "sentence= \"the trade war is expected to continue to have an effect on the\"\n",
    "\n",
    "x = utils.text2tensor(sentence)\n",
    "x = x.unsqueeze(0)\n",
    "\n",
    "scores, (h,c) = net( x.to(device) )\n",
    "\n",
    "print(sentence, '... \\n')\n",
    "\n",
    "p = F.softmax(scores[0][-1],dim=0)\n",
    "\n",
    "utils.show_most_likely_words(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the economy is currently facing a  ... \n",
      "\n",
      "6.1%\t <unk>\n",
      "4.5%\t year\n",
      "2.6%\t new\n",
      "1.9%\t stock\n",
      "1.4%\t series\n",
      "1.4%\t company\n",
      "1.3%\t week\n",
      "1.3%\t deal\n",
      "1.2%\t report\n",
      "1.1%\t recent\n",
      "1.1%\t market\n",
      "1.0%\t federal\n",
      "1.0%\t lot\n",
      "1.0%\t firm\n",
      "0.9%\t month\n",
      "0.9%\t big\n",
      "0.9%\t small\n",
      "0.9%\t time\n",
      "0.8%\t major\n",
      "0.7%\t house\n",
      "0.7%\t large\n",
      "0.7%\t share\n",
      "0.7%\t potential\n",
      "0.6%\t unit\n",
      "0.6%\t law\n",
      "0.6%\t high\n",
      "0.5%\t public\n",
      "0.5%\t day\n",
      "0.5%\t number\n",
      "0.5%\t sale\n"
     ]
    }
   ],
   "source": [
    "sentence= \"the economy is currently facing a \"\n",
    "\n",
    "x = utils.text2tensor(sentence)\n",
    "x = x.unsqueeze(0)\n",
    "\n",
    "scores, (h,c) = net( x.to(device) )\n",
    "\n",
    "print(sentence, '... \\n')\n",
    "\n",
    "p = F.softmax(scores[0][-1],dim=0)\n",
    "\n",
    "utils.show_most_likely_words(p)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the company has been increasing prices in order to drive ... \n",
      "\n",
      "5.7%\t at\n",
      "4.4%\t <unk>\n",
      "3.6%\t sales\n",
      "3.0%\t before\n",
      "2.2%\t down\n",
      "1.9%\t to\n",
      "1.9%\t claims\n",
      "1.7%\t than\n",
      "1.4%\t a\n",
      "1.3%\t as\n",
      "1.1%\t year\n",
      "1.1%\t profit\n",
      "1.0%\t off\n",
      "0.9%\t up\n",
      "0.9%\t was\n",
      "0.9%\t about\n",
      "0.9%\t after\n",
      "0.8%\t lost\n",
      "0.8%\t five\n",
      "0.8%\t modest\n",
      "0.8%\t office\n",
      "0.8%\t nearly\n",
      "0.7%\t within\n",
      "0.7%\t director\n",
      "0.7%\t due\n",
      "0.7%\t with\n",
      "0.7%\t analyst\n",
      "0.7%\t almost\n",
      "0.7%\t two\n",
      "0.6%\t in\n"
     ]
    }
   ],
   "source": [
    "sentence= \"the company has been increasing prices in order to drive\"\n",
    "\n",
    "x = utils.text2tensor(sentence)\n",
    "x = x.unsqueeze(0)\n",
    "\n",
    "scores, (h,c) = net( x.to(device) )\n",
    "\n",
    "print(sentence, '... \\n')\n",
    "\n",
    "p = F.softmax(scores[0][-1],dim=0)\n",
    "\n",
    "utils.show_most_likely_words(p)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
