{"cells": [{"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": "import os\nimport subprocess\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\nimport shutil\nimport pandas as pd\n\npd.set_option(\"max_colwidth\", 100)\nspark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True)\n\nfrom google.cloud import storage"}, {"cell_type": "markdown", "metadata": {}, "source": "# Import & Inspecting Data"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": "directory = 'gs://msca-bdp-tweets/final_project/'\npath = directory"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "156.2 G  156.2 G  gs://msca-bdp-tweets/final_project\n\n"}], "source": "cmd = 'hadoop fs -du -s -h ' + directory\n\np = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, universal_newlines=True)\nfor line in p.stdout.readlines():\n    print (line)\n\nretval = p.wait()"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "CPU times: user 36.5 ms, sys: 18.6 ms, total: 55.1 ms\nWall time: 4min 13s\n"}], "source": "%%time\n\ntweets_df = spark.read.json(path)"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": "# %%time\n\n# tweets_df.count()"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": "# tweets_df.printSchema()"}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"data": {"text/plain": "['contributors',\n 'coordinates',\n 'created_at',\n 'display_text_range',\n 'entities',\n 'extended_entities',\n 'extended_tweet',\n 'favorite_count',\n 'favorited',\n 'filter_level',\n 'geo',\n 'id',\n 'id_str',\n 'in_reply_to_screen_name',\n 'in_reply_to_status_id',\n 'in_reply_to_status_id_str',\n 'in_reply_to_user_id',\n 'in_reply_to_user_id_str',\n 'is_quote_status',\n 'lang',\n 'place',\n 'possibly_sensitive',\n 'quote_count',\n 'quoted_status',\n 'quoted_status_id',\n 'quoted_status_id_str',\n 'quoted_status_permalink',\n 'reply_count',\n 'retweet_count',\n 'retweeted',\n 'retweeted_status',\n 'source',\n 'text',\n 'timestamp_ms',\n 'truncated',\n 'user',\n 'withheld_in_countries']"}, "execution_count": 8, "metadata": {}, "output_type": "execute_result"}], "source": "tweets_df.columns"}, {"cell_type": "code", "execution_count": 26, "metadata": {}, "outputs": [], "source": "# %%time\n\n# tweets_df.select([count(when(col(c).isNull(), c)).alias(c) for c in tweets_df.columns]).show(vertical=True)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# tweets_df.show(1, vertical=True)"}, {"cell_type": "markdown", "metadata": {}, "source": "# Cleaning Data\n#### - Remove the rows with very high null counts\n#### - Removing other cols that shouldnt influence analysis\n#### - Merge extended_tweet Fields (no need for truncated anymore)\n#### - Looking into schema and removing duplicated fields (data that is both in top level and contained in strucs)\n#### - Split timestamp column into separate fields and drop"}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [], "source": "drop_cols = ('contributors','coordinates','display_text_range','extended_entities','withheld_in_countries',\\\n             'lang','possibly_sensitive','in_reply_to_screen_name','in_reply_to_status_id_str','favorited',\\\n             'in_reply_to_user_id_str','quoted_status_id_str','quoted_status_permalink','source','id_str')\n\ntweets_df1 = tweets_df.drop(*drop_cols)\n\ntweets_df2 = tweets_df1.withColumn(\"twt_text_full\",when((col(\"truncated\") == True) & (col(\"extended_tweet\").isNotNull() == True), col('extended_tweet.full_text')).otherwise(col(\"text\")))\\\n                       .withColumn(\"twt_hashtags\",when((col(\"truncated\") == True) & (col(\"extended_tweet\").isNotNull() == True), col('extended_tweet.entities.hashtags.text')).otherwise(col(\"entities.hashtags.text\")))\\\n                       .withColumn(\"twt_country\", col(\"place.country_code\"))\\\n                       .withColumn(\"twt_location_full\", col(\"place.full_name\"))\\\n                       .withColumn(\"twt_location\", col(\"place.name\"))\\\n                       .withColumn(\"twt_location_type\", col(\"place.place_type\"))\\\n                       .withColumn(\"ext_rt_id\",col('retweeted_status.id_str'))\\\n                       .withColumn(\"ext_rt_user_id\",col('retweeted_status.user.id'))\\\n                       .withColumn(\"ext_qt_user_id\",col('quoted_status.user.id'))\\\n                       .withColumn(\"usr_id\", col(\"user.id\"))\\\n                       .withColumn(\"usr_name\", col(\"user.screen_name\"))\\\n                       .withColumn(\"usr_location\", col(\"user.location\"))\\\n                       .withColumn(\"usr_followers\", col(\"user.followers_count\"))\\\n                       .withColumn(\"usr_tweet_count\",col('user.statuses_count'))\\\n                       .withColumn(\"usr_verified\",col('user.verified'))\\\n                       .withColumn(\"usr_desc\",col('user.description'))\\\n                       .withColumn(\"created_timestamp\",from_unixtime(round(col(\"timestamp_ms\")/1000)))\n\ntweets_df2 = tweets_df2.withColumn(\"dt_year\", year(col(\"created_timestamp\")))\\\n                       .withColumn(\"dt_month\", month(col(\"created_timestamp\")))\\\n                       .withColumn(\"dt_day\", dayofmonth(col(\"created_timestamp\")))\\\n                       .withColumn(\"dt_hour\", hour(col(\"created_timestamp\")))\\\n                       .withColumn(\"dt_date\", to_date(col(\"created_timestamp\")))\\\n                       .withColumn(\"dt_datehour\",date_trunc(\"hour\", col(\"created_timestamp\")))\n\ndrop_cols = ('entities','extended_tweet','geo','place','quoted_status','retweeted_status','user','text',\\\n             'truncated','is_quote_status','retweeted','timestamp_ms','created_at'\\\n             'reply_count','retweet_count','quote_count','created_timestamp')\n\ntweets_df3 = tweets_df2.drop(*drop_cols)\n"}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [], "source": "tweets_df3 = tweets_df3.withColumnRenamed(\"in_reply_to_status_id\",\"ext_rp_id\")\\\n                       .withColumnRenamed(\"quoted_status_id\",\"ext_qt_id\")\\\n                       .withColumnRenamed(\"id\",\"twt_id\")\\\n                       .withColumnRenamed(\"filter_level\",\"twt_importance\")\\\n                       .withColumnRenamed(\"favorite_count\",\"twt_likes\")\\\n                       .withColumnRenamed(\"in_reply_to_user_id\",\"ext_rp_user_id\")"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Retwt, reply, and quote counts are 0 since the tweets are grabbed at the time of api\n#### going to make a dataframe of counts of retweeted/quoted/replied tweets and merge with the original dataframe"}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [], "source": "rp_df = tweets_df3.groupBy('ext_rp_id').agg(count('*').alias('twt_reply_count'))\nrt_df = tweets_df3.groupBy('ext_rt_id').agg(count('*').alias('twt_retwt_count'))\nqt_df = tweets_df3.groupBy('ext_qt_id').agg(count('*').alias('twt_quote_count'))\n\ntweets_df3 = tweets_df3.join(rp_df.withColumnRenamed('ext_rp_id','twt_id'), ['twt_id'], how='left')\ntweets_df3 = tweets_df3.join(rt_df.withColumnRenamed('ext_rt_id','twt_id'), ['twt_id'], how='left')\ntweets_df3 = tweets_df3.join(qt_df.withColumnRenamed('ext_qt_id','twt_id'), ['twt_id'], how='left')\n\ntweets_df3 = tweets_df3.na.fill({\"twt_reply_count\": 0,\n                                 \"twt_retwt_count\": 0,\n                                 \"twt_quote_count\": 0})"}, {"cell_type": "markdown", "metadata": {}, "source": "# Finding Relevant Tweets"}, {"cell_type": "markdown", "metadata": {}, "source": "### "}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [], "source": "keywords = ['corona', 'covid', 'sars-cov-2', 'viral', 'wuhan virus', 'china virus',\n            'delta', 'omicron', 'variant', 'vaccin','booster',\n            'moderna','niaid','pfizer','biontech','johnson','j&j',\n            'sinovac','novavax','az','astrazeneca','janssen', 'comirnaty',\n           ]"}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [], "source": "tweets_df_relevant = tweets_df3.select(\"*\", lower(col('twt_text_full')).alias('twt_text'))\n\ntweets_df_relevant = tweets_df_relevant.where(tweets_df_relevant['twt_text'].rlike(\"|\".join([\"(\" + kw + \")\" for kw in keywords])))\n\ntweets_df_relevant = tweets_df_relevant.drop('twt_text_full')\n\ntweets_df_relevant = tweets_df_relevant.withColumn(\"is_original\",when(((col(\"ext_qt_id\").isNull()) & \n                                                                        (col(\"ext_rt_id\").isNull()) & \n                                                                        (col(\"ext_rp_id\").isNull())), 1).otherwise(0))\n\ntweets_df_relevant = tweets_df_relevant.select(sorted(tweets_df_relevant.columns))\n\n# intial_relevant_count = tweets_df_relevant.count()\n# print(intial_relevant_count)"}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [], "source": "tweets_df_remaining = tweets_df3.select(\"*\", lower(col('twt_text_full')).alias('twt_text'))\n\ntweets_df_remaining = tweets_df_remaining.where(~tweets_df_remaining['twt_text'].rlike(\"|\".join([\"(\" + kw + \")\" for kw in keywords])))\n\ntweets_df_remaining = tweets_df_remaining.drop('twt_text_full')\n\ntweets_df_remaining = tweets_df_remaining.withColumn(\"is_original\", lit(0))\n\ntweets_df_remaining = tweets_df_remaining.select(sorted(tweets_df_relevant.columns))\n\n# remaining_count = tweets_df_remaining.count()\n# print(remaining_count)"}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [], "source": "tweets_df_rts = tweets_df_remaining.join(tweets_df_relevant.withColumn(\"ext_rt_id\", col(\"twt_id\")), ['ext_rt_id'], 'leftsemi')\\\n                                   .withColumn(\"is_original\", lit(0))\\\n                                   .select(sorted(tweets_df_relevant.columns))\n\n# relevant_retweet_count = tweets_df_rts.count()\n# print(relevant_retweet_count)"}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [], "source": "tweets_df_qts = tweets_df_remaining.join(tweets_df_relevant.withColumn(\"ext_qt_id\", col(\"twt_id\")), ['ext_qt_id'], 'leftsemi')\\\n                                   .withColumn(\"is_original\", lit(0))\\\n                                   .select(sorted(tweets_df_relevant.columns))\n\n# relevant_quote_count = tweets_df_qts.count()\n# print(relevant_quote_count)"}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [], "source": "tweets_df_rps1 = tweets_df_remaining.join(tweets_df_relevant.withColumn(\"ext_rp_id\", col(\"twt_id\")), ['ext_rp_id'], 'leftsemi')\\\n                                    .withColumn(\"is_original\", lit(0))\\\n                                    .select(sorted(tweets_df_relevant.columns))\n\n# relevant_reply_count1 = tweets_df_rps1.count()\n# print(relevant_reply_count1)"}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [], "source": "tweets_df_rps2 = tweets_df_remaining.join(tweets_df_rps1.withColumn(\"ext_rp_id\", col(\"twt_id\")), ['ext_rp_id'], 'leftsemi')\\\n                                    .withColumn(\"is_original\", lit(0))\\\n                                    .select(sorted(tweets_df_relevant.columns))\n\n# relevant_reply_count2 = tweets_df_rps2.count()\n# print(relevant_reply_count2)"}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "outputs": [], "source": "tweets_df_rps3 = tweets_df_remaining.join(tweets_df_rps2.withColumn(\"ext_rp_id\", col(\"twt_id\")), ['ext_rp_id'], 'leftsemi')\\\n                                    .withColumn(\"is_original\", lit(0))\\\n                                    .select(sorted(tweets_df_relevant.columns))\n\n# relevant_reply_count3 = tweets_df_rps3.count()\n# print(relevant_reply_count3)"}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [], "source": "tweets_df_relevant = tweets_df_relevant.union(tweets_df_rts)\\\n                                       .union(tweets_df_qts)\\\n                                       .union(tweets_df_rps1)\\\n                                       .union(tweets_df_rps2)\\\n                                       .union(tweets_df_rps3)\n\ntweets_df_relevant = tweets_df_relevant.drop_duplicates()"}, {"cell_type": "code", "execution_count": 21, "metadata": {}, "outputs": [], "source": "drop_cols = ('ext_qt_id','ext_rp_id','ext_rp_id')\n\ntweets_df_clean = tweets_df_relevant.drop(*drop_cols)"}, {"cell_type": "code", "execution_count": 22, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- created_at: string (nullable = true)\n |-- dt_date: date (nullable = true)\n |-- dt_datehour: timestamp (nullable = true)\n |-- dt_day: integer (nullable = true)\n |-- dt_hour: integer (nullable = true)\n |-- dt_month: integer (nullable = true)\n |-- dt_year: integer (nullable = true)\n |-- ext_qt_user_id: long (nullable = true)\n |-- ext_rp_user_id: long (nullable = true)\n |-- ext_rt_id: string (nullable = true)\n |-- ext_rt_user_id: long (nullable = true)\n |-- is_original: integer (nullable = false)\n |-- reply_count: long (nullable = true)\n |-- twt_country: string (nullable = true)\n |-- twt_hashtags: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- twt_id: long (nullable = true)\n |-- twt_importance: string (nullable = true)\n |-- twt_likes: long (nullable = true)\n |-- twt_location: string (nullable = true)\n |-- twt_location_full: string (nullable = true)\n |-- twt_location_type: string (nullable = true)\n |-- twt_quote_count: long (nullable = false)\n |-- twt_reply_count: long (nullable = false)\n |-- twt_retwt_count: long (nullable = false)\n |-- twt_text: string (nullable = true)\n |-- usr_desc: string (nullable = true)\n |-- usr_followers: long (nullable = true)\n |-- usr_id: long (nullable = true)\n |-- usr_location: string (nullable = true)\n |-- usr_name: string (nullable = true)\n |-- usr_tweet_count: long (nullable = true)\n |-- usr_verified: boolean (nullable = true)\n\n"}], "source": "tweets_df_clean.printSchema()"}, {"cell_type": "markdown", "metadata": {}, "source": "# Saving to GCS Bucket"}, {"cell_type": "code", "execution_count": 23, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "CPU times: user 555 ms, sys: 41.5 ms, total: 596 ms\nWall time: 8.21 s\n"}], "source": "%%time\n\n# Delete folder from COS bucket\ndef delete_folder(bucket_name, folder_name):\n    gcs_client = storage.Client()\n    bucket = gcs_client.bucket(bucket_name)\n    blobs = list(bucket.list_blobs(prefix=folder_name))\n\n    for blob in blobs:\n        blob.delete()\n\ndelete_folder('msca-bdp-students-bucket', 'shared_data/kaihayden/final1')\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "CPU times: user 422 ms, sys: 184 ms, total: 606 ms\nWall time: 58min 13s\n"}], "source": "%%time\n\ntweets_df_clean.write.format(\"parquet\")\\\n               .mode('overwrite')\\\n               .save('gs://msca-bdp-students-bucket/shared_data/kaihayden/final1')"}, {"cell_type": "code", "execution_count": 27, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "-RECORD 0-----------------------------\n contributors              | 25191000 \n coordinates               | 25188865 \n created_at                | 0        \n display_text_range        | 21497511 \n entities                  | 0        \n extended_entities         | 24418633 \n extended_tweet            | 20859238 \n favorite_count            | 0        \n favorited                 | 0        \n filter_level              | 0        \n geo                       | 25188865 \n id                        | 0        \n id_str                    | 0        \n in_reply_to_screen_name   | 21858171 \n in_reply_to_status_id     | 22009299 \n in_reply_to_status_id_str | 22009299 \n in_reply_to_user_id       | 21858171 \n in_reply_to_user_id_str   | 21858171 \n is_quote_status           | 0        \n lang                      | 0        \n place                     | 25051534 \n possibly_sensitive        | 20548189 \n quote_count               | 0        \n quoted_status             | 19958447 \n quoted_status_id          | 19952378 \n quoted_status_id_str      | 19952378 \n quoted_status_permalink   | 19958447 \n reply_count               | 0        \n retweet_count             | 0        \n retweeted                 | 0        \n retweeted_status          | 7717424  \n source                    | 0        \n text                      | 0        \n timestamp_ms              | 0        \n truncated                 | 0        \n user                      | 0        \n withheld_in_countries     | 25188091 \n\nCPU times: user 43.9 ms, sys: 38.8 ms, total: 82.7 ms\nWall time: 2min 39s\n"}], "source": "%%time\n\ntweets_df.select([count(when(col(c).isNull(), c)).alias(c) for c in tweets_df.columns]).show(vertical=True)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.5"}}, "nbformat": 4, "nbformat_minor": 4}